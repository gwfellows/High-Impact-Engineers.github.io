---
layout: page
title: AI Safety and Governance
permalink: /causes/ai/
authors: Amanda Matthes, Grant Fellows
---

ðŸš§ under construction ðŸš§

## Introduction

[Some experts](https://bluedot.org/blog/agi-timelines) think artificial general intelligence (AGI) could emerge as soon at 2030 due to rapid progress in compute, algorithms, and training techniques. This breakthrough could bring immense gains but also carries potentially [existential risks](https://80000hours.org/agi/#agi-could-bring-huge-benefits-but-poses-incredibly-serious-risks), including [misaligned](https://en.wikipedia.org/wiki/AI_alignment) power-seeking AI, catastrophic misuse such as bioweapons, extreme power concentration, and gradual human disempowerment. EA organizations like [80,000 Hours](https://80000hours.org/agi/) and the [Center for AI Safety](https://safe.ai/) argue these risks are neglected and urgent, making contributions to the AI safety field high-impact.

## How engineers can help
Physical and hardware engineers can help address AI risks by understanding the hardware AI systems are trained on, researching compute scaling trends and supply chains to inform governance and safety planning. Beyond hardware expertise, an engineering mindset is valuable for [AI safety technical research](https://80000hours.org/career-reviews/ai-safety-researcher/). Engineering will also be crucial in securing critical infrastructure against AI-enabled threats and mitigating risks possibly increased by AI such as biological risks.

## Resources to check out

[Profile on becoming an expert in AI hardware](https://80000hours.org/career-reviews/become-an-expert-in-ai-hardware/)

[Engineered for Impact podcast on AI Compute Governance](https://www.youtube.com/watch?v=yK5odT1jz-U)


